---
title: "What sample size to estimate infection prevelance, a simulation study"
author: "Roman Lu≈°trik"
date: "2020-04-18"
output:
  html_document:
    toc: true
    toc_float:
      toc_collapsed: true
    theme: cosmo
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
start <- Sys.time()
```

```{r load_packages, echo = TRUE}
library(ggplot2)
library(parallel)

# Needed for parallel calculations.
cl <- makeCluster(7, outfile = "clusterfuck.txt")
invisible(clusterEvalQ(cl = cl, expr = source("simulation_functions.R")))

source("simulation_functions.R", max.deparse.length = 1000, echo = TRUE)

options(scipen = 14)
```

# No introduction

In order to successfully determine level of prevalence of a disease on a sample of population, we need to gather a sufficiently large sample.

# Algorithm

1. sample `N` individuals from a binomial distribution $Y \sim B(1, myprob)$
    1. if kerning used, introduce false positive and false negative
2. calculate confidence intervals and estimate of prevalence ($\hat{p}$)
3. repeat 1-2 `propN` times $\rightarrow$ `X`
4. calculate proportion of times simulated `myprob` is within the confidence intervals (`within_ci`) and calculate its standard deviations (`sd`) for `X`

Parameters in:

 * simulated probability (`prob`)
 * simulated population size (`N`)
 * number of times simulation is run for each `N` (`propN`)
 
Parameters out:
 
 * lower confidence intervals of $\hat{p}$
 * parameter estimate $\hat{p}$
 * upper confidence interval of $\hat{p}$
 * standard deviation of $\hat{p}$
 * power (in %)

## Kerning

Above algorithm is assuming an ideal situation (what some might consider a spherical chicken in a vacuum). To make things more interesting, let's introduce false positive or false negative rates.

Additional parameters have been introduced, namely `fp` and `fn`.


For each simulation, kerning (adjusting of values) has been performed after N samples has been drawn from $Y \sim B(1, myprob)$. To introduce false positives, a percent of negative cases has been replaced by a vector of equal size where values have been drawn from ($K_{FP} \sim B(1, fp)$). To introduce false negative, a percentage of positive cases has been replaced by a vector of equal size where values have been drawn from $K_{FN} \sim B(1, 1 - fn)$.

# Simulation assumptions

 * sampling is done at random for all variables which influence prevalence (spatial, temporal, demographic, medical...)
 * there are no test shy responders (responders unwilling to take the test for whatever reason)
 * false positive and false negative rates are occurring at random

# Parameters
Parameters start at 0.00075, which is assuming 1500 infected in a population of 2000000. It is probably safe to assume that number of infected or convalesced would not be below this threshold. Sample sizes go from 1000 to 10000. Lower end because this is close to catching one infected individual at assumed prevalence 0.00075 and upper 10000 because this would be in the upper limits of what this author would consider feasible to sample and test in the current climate.
```{r prepare_parameteres}
probs1 <- c(0.00075, 0.001, 0.003, 
            0.005, 0.008, 0.01, 
            0.05, 0.07, 0.1)
propN <- 100
stepN <- seq(from = 1000, to = 10000, by = 1000)
```

# Ideal simulation
In this scenario, there are no false positive or false negative cases. The so called "best case scenario".
```{r simulations_1, warning = FALSE, cache = TRUE}
sim1.params <- expand.grid(N = stepN,
                           myprob = probs1, 
                           propN = propN,
                           fp = 0,
                           fn = 0)
sim1 <- simulateShell(params = sim1.params)
```

```{r plotpower_1, out.width = "\\linewidth"}
plotPower(sim1)
```

Each point represents average of `r propN` simulations where simulated value has been covered by 95 % confidence interval - power.

Power appears to be pretty high for all but for the lowest simulated prevalence values of 0.00075 and 0.001. This indicates that with even at least 1000 samples, best case scenario would yield a viable result.

```{r plotci_1, out.width = "\\linewidth"}
plotCI(sim1)
```
This graph shows mean estimated prevalence and median lower and upper 95 % confidence intervals. Previous figure showed that a sample of 1000 would give us, for a decently large prevalence (above 0.3 %), good power. But this figure shows us that the intervals would be rather large. For example, for prevalence 0.003, 95 % confidence interval could go from 0.001-0.008 (span of 0.007), whereas for a sample size of 3000 (dashed vertical line), the interval would go from 0.0015-0.005 (span of 0.0035, or half of that for $N=1000$).


```{r plot_prob_distro_sim1, out.width = "\\linewidth", fig.width = 9, fig.height = 9, message = FALSE}
plotEstimates(sim1)
```
This figure shows a distribution of estimates around the simulated prevalence value. Notice how estimates take discrete values for low number of samples and low prevalence. Distributions of estimates approximate normal distribution as more data becomes available, which is what we would expect.

# Simulation with kerning

The above simulation is done in a perfect world where sampling is 100 % efficient and tests do not make mistakes. The following simulation will introduce bias. Specifically, we will convert positive cases to negative, simulating false negative rate (`fn`). To keep things simple, no false positives will be introduced.

False negative rate from literature is assumed to be $fn = 0.33$ (worst  case scenario, swab taken 10 days after symptom onset). See [Wikramaratna et al. (2020)](https://www.medrxiv.org/content/10.1101/2020.04.05.20053355v2) for discussion on false negative rates. Briefly, they depend on time and location of where the samples were collected (nasal vs throat swab) while low false positive rates are mainly due to RT-PCR method being highly sensitive and specific (e.g. picking up 10 molecules).

## Using substantial kerning

A pretty pessimistic scenario would be where false negative rate is 0.33 (33 % of positive cases are not detected as such). False positive rate is set to 0 because we want to examine one process at a time.

```{r simulations_2, warning = FALSE, cache = TRUE}
sim2.params <- expand.grid(N = stepN,
                           myprob = probs1, 
                           propN = propN,
                           fp = 0,
                           fn = 0.33)
sim2 <- simulateShell(params = sim2.params)
```

```{r plotpower_2, out.width = "\\linewidth"}
plotPower(sim2)
```

Compared to the idea scenario, power takes a hit for most simulated prevalence values. Power is decreasing with increased sample size. One explanation would be that with more cases, larger portion of cases can potentially be falsely detected as negative cases, yielding underestimated values.

```{r plotci_2, out.width = "\\linewidth", warning = FALSE}
plotCI(sim2)
```

Horizontal dashed line represents simulated prevalence values. Notice how estimates are consistently underestimated. Due to large confidence intervals for smaller sample sizes, the power appears to be higher for these cases, but at a coast of accuracy. Higher prevalence values also means higher negative bias.

```{r plot_estimates_sim2, out.width = "\\linewidth", fig.width = 9, fig.height = 9, message = FALSE}
plotEstimates(sim2)
```
Similar representation as in the previous image, but the distribution of estimates is even more detailed. For small number of data points ($N=1000$, $prob = 0.00075$), values still take rather discrete estimates and as the number of available data points increases (large number samples or higher prevalence), distributions start to approximate normal distribution. Underestimation is evident with estimates aggregating left of the simulated prevalence value (vertical dashed line).

# Using less kerning

Simulating false negative value at 0.1 (10 %).

## FN = 0.1
```{r simulation_less_kerning_1, cache = TRUE}
sim3.params <- expand.grid(N = stepN,
                           myprob = probs1, 
                           propN = propN,
                           fp = 0,
                           fn = 0.1)
sim3 <- simulateShell(params = sim3.params)
```

```{r plotpower_less_kerning_1}
plotPower(sim3)
```

Power improves, especially for simulated prevalence values of 0.1, 0.07 and 0.05.

```{r plotci_less_kerning_1}
plotCI(sim3)
```

Bias is still present, but mostly for the upper end of simulated prevalence values.

```{r plot_estimates_sim3, out.width = "\\linewidth", fig.width = 9, fig.height = 9, message = FALSE}
plotEstimates(sim3)
```
Underestimation of prevalence compared to simulations with $fn=0.33$ has decreased.

## FN = 0.01

Examine if bias in underestimation of prevalence decreases even further.

```{r simulation_less_kerning_2, cache = TRUE}
sim4.params <- expand.grid(N = stepN,
                           myprob = probs1, 
                           propN = propN,
                           fp = 0,
                           fn = 0.01)
sim4 <- simulateShell(params = sim4.params)
```

```{r plotpower_less_kerning_2}
plotPower(sim4)
```

```{r plotci_less_kerning_2}
plotCI(sim4)
```
```{r plot_estimates_sim4, out.width = "\\linewidth", fig.width = 9, fig.height = 9, message = FALSE}
plotEstimates(sim4)
```
It does.

## FN = 0.001

More power to the engines! False negative rate set to 0.001.

```{r simulation_less_kerning_3, cache = TRUE}
sim5.params <- expand.grid(N = stepN,
                           myprob = probs1, 
                           propN = propN,
                           fp = 0,
                           fn = 0.001)
sim5 <- simulateShell(params = sim5.params)
```

```{r plotpower_less_kerning_3}
plotPower(sim5)
```


```{r plotci_less_kerning_3}
plotCI(sim5)
```
```{r plot_estimates_sim5, out.width = "\\linewidth", fig.width = 9, fig.height = 9, message = FALSE}
plotEstimates(sim5)
```

Bias is not apparent, results appear similar to values of the ideal simulation.

# Alternate view of the data

Below images shows `ideal` and `intervention` (with kerning) simulations side by side for all simulated prevalence values. Notice the drop in power for simulated prevalence numbers for high false negative rates in conjunction with large sample sizes. My guess would be that this happens because for smaller sample size, kernining has a relatively small effect and as the sample size increases.
```{r plot_combined_image, out.width = "\\linewidth"}
sim1summ <- summarizeSimulation(sim1)
sim1summ$source <- "ideal"
sim2summ <- summarizeSimulation(sim2)
sim2summ$source <- "fn=0.33"
sim3summ <- summarizeSimulation(sim3)
sim3summ$source <- "fn=0.1"
sim4summ <- summarizeSimulation(sim4)
sim4summ$source <- "fn=0.01"
sim5summ <- summarizeSimulation(sim5)
sim5summ$source <- "fn=0.001"

sims <- do.call(rbind, list(sim1summ, sim2summ, sim3summ,
                            sim3summ, sim4summ, sim5summ))

ggplot(sims, aes(x = N, y = powerin, color = source)) +
        theme_bw() +
        theme(legend.position = "top", legend.direction = "horizontal") +
        scale_color_brewer(palette = "Set1", name = "simulated conditions") +
        scale_x_continuous(breaks = seq(0, 10000, by = 2000)) +
        geom_vline(xintercept = 3000, color = "grey60", linetype = "dashed") +
        geom_line() +
        geom_point(size = 2, alpha = 0.75) +
        facet_wrap(~ prob)
```

```{r plot_histogram_total, out.width = "\\linewidth", fig.width = 9, fig.height = 9}
ggplot(do.call(rbind, list(sim1, sim2, sim3, sim4, sim5)), 
       aes(fill = as.factor(fn))) +
  theme_bw() +
  theme(legend.position = "top", 
        axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5)) +
  scale_fill_brewer(palette = "Set1", name = "Simulated bias (FN)") +
  geom_histogram(aes(x = y), alpha = 0.7, position = "identity") +
  geom_vline(aes(xintercept = prob), color = "grey60", linetype = "dashed") +
  facet_grid(N ~ prob, scale = "free_x")
```

There appears to be a sweet spot of false negative rate where its effect is still negligible to the overall experiment, assuming all other assumptions hold. Based on the above result, this would be in single digits.

# Used packages
```{r}
sessionInfo()
end <- Sys.time()
simtime <- end - start
```

Processing took `r round(simtime, 3)` `r attr(simtime, "units")` (results may have been cached, so your experience may differ).